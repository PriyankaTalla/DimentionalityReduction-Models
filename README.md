## Below are the list of Dimentionality Reduction Models covered in this repository with examples

1. Principal Component Analysis (PCA)
Concepts: Variance maximization, orthogonal transformation.
Mathematical Topics:
Linear Algebra: Eigenvalues, eigenvectors, covariance matrix.
Statistics: Variance, explained variance ratio.
Example: Reducing the dimensions of a dataset with multiple features to visualize it in 2D.

2. Singular Value Decomposition (SVD)
Concepts: Matrix factorization, dimensionality reduction.
Mathematical Topics:
Linear Algebra: Singular values, U, Î£, and V^T matrices.
Statistics: Variance capture by singular values.
Example: Reducing the dimensionality of a document-term matrix in text mining.

3. Independent Component Analysis (ICA)
Concepts: Statistical independence, source separation.
Mathematical Topics:
Probability: Kurtosis, negentropy.
Statistics: Independence, central limit theorem.
Example: Separating mixed signals in blind source separation problems (e.g., cocktail party problem).

4. t-Distributed Stochastic Neighbor Embedding (t-SNE)
Concepts: Manifold learning, preserving local structure.
Mathematical Topics:
Probability: Joint probabilities, Gaussian distributions.
Optimization: Gradient descent.
Example: Visualizing high-dimensional data such as images in a 2D plot.

5. Linear Discriminant Analysis (LDA)
Concepts: Maximize class separability, supervised learning.
Mathematical Topics:
Statistics: Between-class and within-class scatter matrices.
Linear Algebra: Eigenvalues, eigenvectors.
Example: Reducing dimensions of a labeled dataset for classification tasks.

6. Kernel PCA
Concepts: Nonlinear dimensionality reduction, kernel trick.
Mathematical Topics:
Linear Algebra: Eigenvalues, eigenvectors.
Statistics: Covariance matrix, kernel functions.
Example: Reducing dimensions of non-linearly separable data.

7. Isomap
Concepts: Nonlinear dimensionality reduction, geodesic distances.
Mathematical Topics:
Graph Theory: Shortest paths, neighborhood graphs.
Linear Algebra: Eigenvalues, eigenvectors.
Example: Reducing dimensions of data lying on a nonlinear manifold.

8. Locally Linear Embedding (LLE)
Concepts: Manifold learning, preserving local neighborhoods.
Mathematical Topics:
Linear Algebra: Eigenvalues, eigenvectors.
Optimization: Reconstruction weights.
Example: Unfolding a Swiss roll dataset to 2D.

9. Autoencoders
Concepts: Neural networks for dimensionality reduction.
Mathematical Topics:
Optimization: Backpropagation, loss functions.
Linear Algebra: Matrix multiplications, activation functions.
Example: Compressing images into a lower-dimensional representation.

